<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head> 
<title>MVA Audio Signal Processing Course </title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <style>
        body {
            font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
            line-height: 1.6;
            margin: 2.5rem auto;
            max-width: 1000px;
            color: #111;
        }

        h1, h2, h3 {
            font-weight: 600;
            line-height: 1.25;
        }

        h1 {
            text-align: center;
            margin-bottom: 2.5rem;
        }

        h2 {
            margin-top: 3rem;
            padding-bottom: 0.3rem;
            border-bottom: 2px solid #ddd;
        }

        h3 {
            margin-top: 0;
        }

        p {
            margin: 0.75rem 0;
        }

        ul {
            margin: 0.5rem 0 1rem 1.2rem;
        }

        li {
            margin: 0.3rem 0;
        }

        a {
            color: #0033cc;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .warning {
            border-left: 5px solid #b00020;
            background: #fff5f6;
            padding: 1rem 1.25rem;
            margin: 2rem 0;
        }

        .section {
            margin-top: 2rem;
        }

        #projects > p {
            margin: 1rem 0 2rem 0;
            font-style: italic;
        }

        article {
            background: #fafafa;
            border: 1px solid #ddd;
            padding: 1.25rem 1.5rem;
            margin: 2rem 0;
        }

        article h3 {
            margin-bottom: 0.5rem;
        }

        article strong {
            font-weight: 600;
        }

        hr {
            display: none;
        }
    </style>

</head>

<body>
<h1>
<p align="center">MVA Material for the course on Audio Signal Processing</p>
<p>&nbsp;</p>
</h1>

<p>&nbsp;</p>
<h2> Registration to the course : </h2>
<ul>
    <li> Please fill up this  <a href="https://forms.gle/r2iEzcm3AXkzv8vw9"> form </a> before Februray 7th to notify your registration </li>
</ul>

<h2>Course outline </h2>
<p><strong> WARNING :  The first introductory lecture is highly audio-based, with many examples, and introduces important concepts. </strong></p>
<ul>
  <li>Part I : Introduction <em><a href="Part I.pdf" target="_blank">(slides</a>)</em></li>
</ul>
<ul>
  <li>Part II : Analogic signal/Digital signal <em><a href="Part II.pdf">(slides</a>)</em></li>
</ul>
<ul>
  <li>Part III : Filtering <em><a href="Part III.pdf">(slides</a>)</em>&nbsp;</li>
</ul>
<ul>
  <li>Part IV : Stochastic signal processing <em><a href="Part IV.pdf">(slides</a>)</em>&nbsp;</li>
</ul>
<ul>
  <li>Part V : Speech processing <em><a href="Part V.pdf">(slides</a>)</em></li>
</ul>
<ul>
  <li>Part VI : Denoising <em><a href="Part VI.pdf">(slides</a>)</em></li>
</ul>
<ul>
  <li>Part VII : Time-frequency analysis <em><a href="Part VII.pdf">(slides</a>)</em></li>
</ul>
<p><strong> The slides for each lecture might be updated before the lecture.  </strong>:  </p>

<p><strong> Reading the course slides only is not enough to properly understand the material  </strong>  </p>

<h2>Course Validation</h2>

<div class="section">
    <p><strong>For the validation of the course, you must complete a project:</strong></p>
    <ul>
        <li>Each project can be done in pairs (2 students).</li>
        <li>Calibrated for approximately <strong>15 hours</strong> of work.</li>
        <li>Each project is associated with a research paper.</li>
        <li>The list of projects is provided below, with the expected deliverables for each.</li>
        <li>A written report is required for each project.</li>
    </ul>
</div>

<div class="section">
    <p><strong>There will be a 25-minute oral presentation:</strong></p>
    <ul>
        <li>You will present your project.</li>
        <li>Discussion around your work.</li>
        <li>One course question related to your project.</li>
    </ul>
</div>

<section id="projects">
    <h2>Projects</h2>

    All projects involve deep learning components. Their purpose is to place classical signal processing methods (e.g., filtering, spectral analysis, linear prediction, parametric synthesis) in perspective by directly comparing them with modern learning-based approaches addressing the same tasks.

    <!-- PROJECT 1 -->
    <article>
        <h3>Project 1 - SEGAN: Speech Enhancement Generative Adversarial Network</h3>
        <p><strong>Paper:</strong> Pascual et al., <em>SEGAN: Speech Enhancement Generative Adversarial Network</em><br/>
            <a href="https://arxiv.org/abs/1703.09452" target="_blank">https://arxiv.org/abs/1703.09452</a>
        </p>

        <p><strong>Objective of the work</strong><br/>
            The goal of this project is to study speech denoising directly in the time domain using a convolutional neural network.
            The work focuses on understanding what a learned filter can achieve compared to a classical Wiener filter under controlled assumptions.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Generate noisy speech signals by adding stationary noise to clean speech.</li>
            <li>Implement the SEGAN generator only (1D U-Net), without the adversarial discriminator.</li>
            <li>Train the generator using an L1 or L2 loss on <strong> short </strong> patches.</li>
            <li>Implement a Wiener filter (with a sliding window) assuming noise estimation from an initial silence segment.</li>
            <li>Compare Wiener and SEGAN outputs using spectrograms and listening tests.</li>
        </ul>
    </article>

    <hr/>

    <!-- PROJECT 2 -->
    <article>
        <h3>Project 2 - DCCRN: Deep Complex Convolution Recurrent Network</h3>
        <p><strong>Paper:</strong> Hu et al., <em>DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement</em><br/>
            <a href="https://arxiv.org/abs/2008.00264" target="_blank">https://arxiv.org/abs/2008.00264</a>
        </p>

        <p><strong>Objective</strong><br/>
            This project investigates the role of phase information in speech denoising.
            The goal is to compare magnitude-only Wiener or deep learning approaches with complex-valued deep-learning approach.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Generate noisy speech signals by adding stationary noise to clean speech.</li>
            <li>Implement a Wiener filter (with a sliding window) assuming noise estimation from an initial silence segment.</li>
            <li>Implement an STFT processing pipeline.</li>
            <li>Train a magnitude-only neural CNN network for speech denoising.</li>
            <li>Train a simplified complex-valued CNN network using both real and imaginary parts of STFT .</li>
            <li>Compare all methods in terms of spectrograms, phase behavior, and perceptual quality.</li>
        </ul>
    </article>

    <hr/>

    <!-- PROJECT 3 -->
    <article>
        <h3>Project 3 - MetricGAN: Optimizing Perceptual Metrics</h3>
        <p><strong>Paper:</strong> Fu et al., <em>MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement</em><br/>
            <a href="https://arxiv.org/abs/1905.04874" target="_blank">https://arxiv.org/abs/1905.04874</a>
        </p>

        <p><strong>Objective</strong><br/>
            This project studies the impact of the loss function in deep learning-based speech denoising.
            The architecture is fixed and simple; only the training objective is modified.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Generate noisy speech signals by adding stationary noise to clean speech.</li>
            <li>Implement a simple STFT-based CNN denoiser using only the magnitude of the STFT .</li>
            <li>Train the denoiser using a classical MSE or L1 loss.</li>
            <li>Pre-train a lightweight neural network to approximate a perceptual speech quality metric.</li>
            <li>Train the CNN denoiser model using this perceptual metric-based loss.</li>
            <li>Analyze differences between energy-based and perceptual optimization.</li>
        </ul>
    </article>

    <hr/>

    <!-- PROJECT 4 -->
    <article>
        <h3>Project 4 - DDSP: Differentiable Digital Signal Processing</h3>
        <p><strong>Paper:</strong> Engel et al., <em>DDSP: Differentiable Digital Signal Processing</em><br/>
            <a href="https://arxiv.org/abs/2001.04643" target="_blank">https://arxiv.org/abs/2001.04643</a>
        </p>

        <p><strong>Objective</strong><br/>
            The goal of this project is to compare analytical standard (DSP) pipeline sound synthesis with learned parameter estimation.
            Students study how neural networks can be used to predict parameters of a classical harmonic-plus-noise synthesizer.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Implement a complete DSP analysis-synthesis pipeline, including:
                <ul>
                    <li>Implement analysis/estimation on sliding windows :</li>
                <ul>
                    <li>Pitch estimation and voiced/unvoiced decision using auto-correlation or cepstrum or Yin methods
                        (you can copy/paste existing code, but if you do so you are expected to master it) </li>
                    <li>Harmonic analysis based on the estimated pitch (harmonic frequencies and amplitudes). </li>
                    <li>Noise analysis by separating harmonic and residual components in the STFT domain
                        using a simple harmonic mask, and modeling the residual noise through its spectral envelope.</li>
                    <li> Estimation of the overall smooth spectral envelope (e.g., LPC or cepstral smoothing).</li>
                </ul>
                    <li>Then implement signal reconstruction : (harmonic part + noise part) filtered by overall filter.</li>
        </ul>
            </li>
            <li>Implement a DDSP version of the same synthesizer: keep the DSP synthesis model fixed, but replace the parameter estimation
                with a a small frame-wise regular feed-forward NN (no recurrent or attention-based architecture is required) that predicts the synthesizer parameters end-to-end.
            </li>
            <li>Train the network by minimizing an audio reconstruction loss between the synthesized output and the original signal on a small dataset (single instrument or voice).</li>
            <li>Comparisons</li>
        </ul>
    </article>

    <hr/>

    <!-- PROJECT 5 -->
    <article>
        <h3>Project 5 - LPCNet: Neural Excitation for LPC Vocoding</h3>
        <p><strong>Paper:</strong> Valin & Skoglund, <em>LPCNet: Improving Neural Speech Synthesis through Linear Prediction</em><br/>
            <a href="https://arxiv.org/abs/1810.11846" target="_blank">https://arxiv.org/abs/1810.11846</a>
        </p>

        <p><strong>Objective</strong><br/>
            This project analyzes what neural networks learn when combined with a classical LPC vocoder.
            The focus is exclusively on excitation modeling rather than spectral envelope estimation.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Implement a classical LPC analysis?synthesis vocoder with heuristic excitation (dirac trains or white noise).</li>
            <li>Implement an LPC analysis?synthesis vocoder using the true residual (obtained by substraction).</li>
            <li>Fine-tune a pre-trained LPCNet model on a small, new speaker dataset (short patches).</li>
            <li>Compare the three appraoches above.</li>
            <li>Discuss the limits of LPC modeling and the contribution of learning.</li>
        </ul>
    </article>

    <hr/>

    <!-- PROJECT 6 -->
    <article>
        <h3>Project 6 - CREPE: Deep Learning for Pitch Estimation</h3>
        <p><strong>Paper:</strong> Kim et al., <em>CREPE: A Convolutional Representation for Pitch Estimation</em><br/>
            <a href="https://arxiv.org/abs/1802.06182" target="_blank">https://arxiv.org/abs/1802.06182</a>
        </p>

        <p><strong>Objective</strong><br/>
            This project compares classical DSP pitch estimation methods with a deep learning-based approach.
            The emphasis is on experimental evaluation, robustness, and voiced/unvoiced decision making.
        </p>

        <p><strong>Expected work</strong></p>
        <ul>
            <li>Reimplement two DSP pitch estimation methods (e.g., autocorrelation and YIN).</li>
            <li>Use the pre-trained CREPE model for pitch estimation.</li>
            <li>Design an ML (DL if you want) algorithm to evaluate the voiced/unvoiced decision rule based on CREPE outputs.</li>
            <li>Run a fixed experimental protocol:
                <ul>
                    <li>Voiced clean speech.</li>
                    <li>Unvoiced speech segments.</li>
                    <li>Voiced speech with stationary noise at SNR = 20 dB, 10 dB, and 0 dB.</li>
                    <li>Synthetic tonal signals with known ground-truth pitch.</li>
                    <li> ... </li>
                </ul>
            </li>
            <li>Compare DSP and deep learning methods using the following mandatory criteria:
                <ul>
                    <li>Pitch estimation error on synthetic signals.</li>
                    <li>Pitch stability on sustained voiced segments.</li>
                    <li>Voiced / unvoiced classification errors (false positives and false negatives).</li>
                    <li>Qualitative analysis of typical failure cases.</li>
                    <li> ... </li>
                </ul>
            </li>
        </ul>
    </article>

</section>



</body>
</html>
